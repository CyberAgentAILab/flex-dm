<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>FlexDM</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://cyberagentailab.github.io/flex-dm/images/teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="1024">
    <meta property="og:image:height" content="718">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://cyberagentailab.github.io/layout-dm"/>
    <meta property="og:title" content="Towards Flexible Multi-modal Document Models" />
    <meta property="og:description" content="Creative workflows for generating graphical documents involve complex inter-related tasks, such as aligning elements, choosing appropriate fonts, or employing aesthetically harmonious colors. In this work, we attempt at building a holistic model that can jointly solve many different design tasks. Our model, which we denote by FlexDM, treats vector graphic documents as a set of multi-modal elements, and learns to predict masked fields such as element type, position, styling attributes, image, or text, using a unified architecture. Through the use of explicit multi-task learning and in-domain pre-training, our model can better capture the multi-modal relationships among the different document fields. Experimental results corroborate that our single FlexDM is able to successfully solve a multitude of different design tasks, while achieving performance that is competitive with task-specific and costly baselines." />

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Towards Flexible Multi-modal Document Models" />
    <meta name="twitter:description" content="Creative workflows for generating graphical documents involve complex inter-related tasks, such as aligning elements, choosing appropriate fonts, or employing aesthetically harmonious colors. In this work, we attempt at building a holistic model that can jointly solve many different design tasks. Our model, which we denote by FlexDM, treats vector graphic documents as a set of multi-modal elements, and learns to predict masked fields such as element type, position, styling attributes, image, or text, using a unified architecture. Through the use of explicit multi-task learning and in-domain pre-training, our model can better capture the multi-modal relationships among the different document fields. Experimental results corroborate that our single FlexDM is able to successfully solve a multitude of different design tasks, while achieving performance that is competitive with task-specific and costly baselines." />
    <meta name="twitter:image" content="https://cyberagentailab.github.io/flex-dm/images/teaser.png" />

    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-115292904-1', 'auto');
        ga('send', 'pageview');
    </script>
    <!-- <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’«</text></svg>"> -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">
    <!-- <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@48,400,0,0" /> -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Towards Flexible Multi-modal Document Models </br>
                <small>CVPR 2023 (highlight) </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li><a href="https://naoto0804.github.io/">Naoto Inoue</a><br>CyberAgent</br></li>
                    <li><a href="https://ktrk115.github.io/">Kotaro Kikuchi</a><br>CyberAgent</br></li>
                    <li><a href="https://esslab.jp/~ess/en/">Edgar Simo-Serra</a><br>Waseda University</br></li>
                    <li><a href="https://mayu-ot.github.io/">Mayu Otani</a><br>CyberAgent</br></li>
                    <li><a href="https://sites.google.com/view/kyamagu">Kota Yamaguchi</a><br>CyberAgent</br></li>
                </ul>
            </div>
        </div>

        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="icon/description.svg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/CyberAgentAILab/flex-dm">
                            <image src="icon/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>

        <br>

        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="row">
                    <p class="text-center">
                        <img src="images/teaser.png" class="img-rounded" width="100%">
                    </p>
                    <p class="text-justify">
                        Examples of the design tasks that can be solved by our proposed FlexDM model, which is designed to process a vector graphic document consisting of an arbitrary number of elements (e.g., text).
                        Each element is composed of multi-modal fields indicating its attribute properties (e.g., text content, position, font color, etc.).
                    </p>
                </div>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Creative workflows for generating graphical documents involve complex inter-related tasks, such as aligning elements, choosing appropriate fonts, or employing aesthetically harmonious colors. In this work, we attempt at building a holistic model that can jointly solve many different design tasks. Our model, which we denote by FlexDM, treats vector graphic documents as a set of multi-modal elements, and learns to predict masked fields such as element type, position, styling attributes, image, or text, using a unified architecture. Through the use of explicit multi-task learning and in-domain pre-training, our model can better capture the multi-modal relationships among the different document fields. Experimental results corroborate that our single FlexDM is able to successfully solve a multitude of different design tasks, while achieving performance that is competitive with task-specific and costly baselines.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Model
                </h3>
                <p class="text-justify">
                    We encode a vector graphic document into a 2D array of latent vectors.
                    We perform <strong>masked field prediction</strong> as we do in images or texts.
                    This way we can utilize masking patterns to switch among various design tasks within a single model.
                </p>
                <div class="row">
                    <p class="text-center">
                        <img src="images/overview_concepts_wider.png" class="img-rounded" width="100%">
                    </p>
                    <p class="text-justify">
                        <strong>Left</strong>: example of a vector graphic document.
                        Each row / column in the array cossponds to a single element / an attribute or a group of attributes. <br>
                        <strong>Right</strong>: Correspondence between design tasks and masking patterns for our masked field prediction
                    </p>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
                <p class="text-justify">
                    For more results, please refer to the <a href="pdfs/supplementary.pdf">supplementary material</a>.
                </p>
                <div class="row">
                    <p class="text-center">
                        <img src="images/crello_prediction.png" class="img-rounded" width="100%">
                    </p>
                    <p class="text-justify">
                        Prediction of our FlexDM model in crello dataset. [MASK] is visualized using default values.
                    </p>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-12"><pre>
@inproceedings{inoue2023layout,
    title={{Towards Flexible Multi-modal Document Models}},
    author={Naoto Inoue and Kotaro Kikuchi and Edgar Simo-Serra and Mayu Otani and Kota Yamaguchi},
    booktitle={The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2023},
    pages={XXXX-XXXX},
    doi={XXXX}
  }</pre>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                The website template was borrowed from <a href="https://jonbarron.info/mipnerf360/">Mip-NeRF 360</a>.
                </p>
            </div>
        </div>

        <!-- <h3>Resources</h3>
        <div class="row">
            <div class="col-md-12">
                <ul>
                    <li><a href="https://arxiv.org/abs/1803.11365">arXiv</a></li>
                    <li><a href="https://openaccess.thecvf.com/content_cvpr_2018/html/Inoue_Cross-Domain_Weakly-Supervised_Object_CVPR_2018_paper.html">proceedings</a></li>
                    <li><a href="https://github.com/naoto0804/cross-domain-detection">code/dataset</a></li>
                    <li><a href="https://drive.google.com/file/d/1AXhwDfgzgfMD8W2s4y1f8rUSzErsL5yg/view">poster</a></li>
                    <li><a href="https://drive.google.com/file/d/1v1IJDCJpVBjQhNZY3nSGJcfVX6tFo4ho/view">slide(ja)</a></li>
                </ul>
            </div>
        </div> -->
    </div>
    <footer class="text-center">
        &#169; Naoto Inoue<br />
        Last Update: 2023-03-30
    </footer>
</body>

</html>